import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import math
from main import OUActionNoise
from main import Buffer


dt = 0.1

rho = 0.3

ve = 0.8

#angle between initial velocity and reference
te = 3*np.pi/4

def transition_e_sl(pursuer_state, state, u): 
    
    #angle between velocity and reference axis
    theta_velocity_p = thetap(state) 
    
    old_phi = phi(state)

    #Change in x and y position of Pursuer
    del_xp = state[2]*dt
    del_yp = state[3]*dt
    
    #Change in x and y position of Evader
    del_xe = ve*np.cos(te)*dt
    del_ye = ve*np.sin(te)*dt
    
    #Change in Angle between velocity and reference axis
    del_theta_velocity_p = (v/rho)*u*dt
    theta_velocity_p = theta_velocity_p + del_theta_velocity_p

    #New x and y position of Pursuer
    state[0] = state[0] + del_xp
    state[1] = state[1] + del_yp
    
    #New x and y position of Evader
    state[4] = state[4] + del_xe
    state[5] = state[5] + del_ye
    
    #New x and y velocity of Pursuer
    state[2] = 1.0*np.cos(theta_velocity_p)
    state[3] = 1.0*np.sin(theta_velocity_p)
    
    new_phi = phi(state)

    p_state = [0,0,0]
    
    #New phi and phi(dot)
    p_state[1] = phi(state)/np.pi
    p_state[0] = L(state)/30.0
    p_state[2] = (new_phi-old_phi)/dt
    return p_state, state

def policy(state, noise_object):
    sampled_actions = tf.squeeze(actor_model(state))
    noise = noise_object()
    sampled_actions = sampled_actions.numpy()
    return [sampled_actions]

ep_reward_list = []
# To store average reward history of last few episodes
avg_reward_list = []

# Takes about 20 min to train
for ep in range(1):

    sys_state = [0.0,0.0,1.0,0.0,10.0,10.0]
    prev_state = np.array([L(sys_state)/30.0, phi(sys_state)/np.pi,0])#, abs(alph(sys_state))/(2*np.pi)])
    episodic_reward = 0
    
    xc = []
    yc = []
    
    xce = []
    yce = []

    
    #while True:
    for i in range(1000):
        
        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)

        
        action = policy(tf_prev_state, ou_noise)
        
        # Recieve state and reward from environment.
        new_state, sys_state = transition_e_sl(prev_state, sys_state, float(action[0]))
        rew = reward(prev_state)
        episodic_reward += rew        
        if L(sys_state) <= 0.2:
            print ('****TARGET CAPTURED****')
            print('Captured after', i+1, 'steps')
            break
                
        prev_state = new_state
        xc.append(sys_state[0])
        yc.append(sys_state[1])
        
        xce.append(sys_state[4])
        yce.append(sys_state[5])
        
    xc1 = [sys_state[4]]
    yc1 = [sys_state[5]]

    ep_reward_list.append(episodic_reward)

    # Mean of last 40 episodes
    avg_reward = np.mean(ep_reward_list[-40:])
    print("Episode * {} * Avg Reward is ==> {}".format(ep, avg_reward))
    avg_reward_list.append(avg_reward)
    plt.plot(xc,yc, label = 'Pursuer Trajectory')
    plt.plot(xce,yce, label = 'Target Trajectory')
    plt.plot(xc1,yc1,'*', label = 'Target Final Position')
    plt.legend()
    title = 'Episode Number', ep+1
    plt.title(title)
    plt.grid()
    plt.show()

'''
*************************************
Model Cheker for straight line evader
*************************************
'''

'''
*************************************
Model Cheker for maneuvering evader
*************************************
'''
dt = 0.1

ve = 0.9

#angle between initial velocity and reference
te = 3*np.pi/4

#Control Variable for evader
ue = 0.1

#Minimum Turning radius of evader
rhoe = 2

def transitionme(pursuer_state, state, u, fake_state, theta_velocity_e, theta_velocity_p): 

    old_phi = phi(state)

    #Change in x and y position of Pursuer
    del_xp = state[2]*dt
    del_yp = state[3]*dt
    
    #Change in x and y position of Evader
    del_xe = fake_state[2]*dt
    del_ye = fake_state[3]*dt
    
    #Change in Angle between velocity and reference axis
    del_theta_velocity_p = (v/rho)*u*dt
    theta_velocity_p = theta_velocity_p + del_theta_velocity_p
    
    #Change in Angle between evader velocity and reference axis
    del_theta_velocity_e = (ve/rhoe)*ue*dt
    theta_velocity_e = theta_velocity_e + del_theta_velocity_e
    
    #New x and y position of Pursuer
    state[0] = state[0] + del_xp
    state[1] = state[1] + del_yp
    
    #New x and y position of Evader
    state[4] = state[4] + del_xe
    state[5] = state[5] + del_ye
    
    #New x and y velocity of Pursuer
    state[2] = 1.0*np.cos(theta_velocity_p)
    state[3] = 1.0*np.sin(theta_velocity_p)
    
    v_e = [0,0]
    #New velocity of evader
    v_e[0] = ve*np.cos(theta_velocity_e)
    v_e[1] = ve*np.sin(theta_velocity_e)
    
    fake_state[2] = v_e[0]
    fake_state[3] = v_e[1]
    
    new_phi = phi(state)

    p_state = [0,0,0]
    
    #New phi and phi(dot)
    p_state[1] = phi(state)/np.pi
    #p_state[1] = (del_theta_velocity_p/dt)/(2*np.pi)
    p_state[0] = L(state)/30.0
    p_state[2] = (new_phi-old_phi)/dt
    
    return p_state, state, fake_state, theta_velocity_e, theta_velocity_p

def policy(state, noise_object):
    sampled_actions = tf.squeeze(actor_model(state))
    noise = noise_object()
    # Adding noise to action
    sampled_actions = sampled_actions.numpy() #+ noise

    return [sampled_actions]

ep_reward_list = []
# To store average reward history of last few episodes
avg_reward_list = []

# Takes about 20 min to train
for ep in range(1):

    sys_state = [0.0,0.0,1.0,0.0,5.0,5.0]
    prev_state = np.array([L(sys_state)/30.0, phi(sys_state)/np.pi,0])#, abs(alph(sys_state))/(2*np.pi)])
    
    velocity_evader = [ve*np.cos(te), ve*np.sin(te)]
    
    fake_state = [0.0, 0.0, velocity_evader[0], velocity_evader[1], 0.0, 0.0]
    
    theta_velocity_e = thetap(fake_state)
    theta_velocity_p = thetap(sys_state)
    
    
    episodic_reward = 0
    
    xc = []
    yc = []
    
    xce = []
    yce = []

    
    #while True:
    for i in range(1000):
        
        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)    
        action = policy(tf_prev_state, ou_noise)
        # Recieve state and reward from environment.
        new_state, sys_state, fake_state, theta_velocity_e, theta_velocity_p = transitionme(prev_state, sys_state, float(action[0]), fake_state, theta_velocity_e, theta_velocity_p)
        rew = reward(new_state)
        episodic_reward += rew
                
        if L(sys_state) <= 0.2:
            print ('****TARGET CAPTURED****')
            print('Captured after', i+1, 'steps')
            break
                
        prev_state = new_state
        xc.append(sys_state[0])
        yc.append(sys_state[1])
        
        xce.append(sys_state[4])
        yce.append(sys_state[5])
        
    xc1 = [sys_state[4]]
    yc1 = [sys_state[5]]

    ep_reward_list.append(episodic_reward)

    # Mean of last 40 episodes
    avg_reward = np.mean(ep_reward_list[-40:])
    print("Episode * {} * Avg Reward is ==> {}".format(ep, avg_reward))
    avg_reward_list.append(avg_reward)
    plt.plot(xc,yc, label = 'Pursuer Trajectory')
    plt.plot(xce,yce, label = 'Target Trajectory')
    plt.plot(xc1,yc1,'*', label = 'Target Final Position')
    plt.legend()
    title = 'Episode Number', ep+1
    plt.title(title)
    plt.show()

'''
*************************************
Model Cheker for maneuvering evader
*************************************
'''
